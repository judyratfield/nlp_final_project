{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "716e7a89",
   "metadata": {},
   "source": [
    "5) Classify - combines solutions from 1-4 to classify sentiment of customer and to which agent should the customer be assigned to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc44d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from IPython import get_ipython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def import_from_notebook(notebook_path, function_name):\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "    shell = InteractiveShell.instance()\n",
    "    code = \"\\n\".join([cell.source for cell in nb.cells if cell.cell_type == 'code'])\n",
    "    exec(code, shell.user_ns)\n",
    "    return shell.user_ns[function_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "425b00d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "WARNING:tensorflow:From C:\\Users\\rodul\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing. Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: {'text': \" I'm dissatisfied with the technician who helped me yesterday. I'm unhappy with your service. I actually have a problem with the way you handled my request. This is unacceptable. I would like to speak with your supervisor. Fuck you! I hate you for treating me like shit!\"}\n",
      "User is angry.\n",
      "Transcribing. Please wait...\n",
      "Transcript: {'text': \" I'm dissatisfied with the technician who helped me yesterday. I'm unhappy with your service. I actually have a problem with the way you handled my request. This is unacceptable. I would like to speak with your supervisor. Fuck you! I hate you for treating me like shit!\"}\n",
      "User is angry.\n"
     ]
    }
   ],
   "source": [
    "is_caller_angry = import_from_notebook('Text Semantic Analysis.ipynb', 'is_caller_angry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4acaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Load and process the audio files in a folder\n",
    "def load_audio_file(file_path):\n",
    "    data, sampling_rate = librosa.load(file_path, duration=2.5, offset=0.6)\n",
    "    return data, sampling_rate\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(data, sample_rate):\n",
    "    result = np.array([])\n",
    "    \n",
    "    # Zero Crossing Rate\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    result = np.hstack((result, zcr))\n",
    "    \n",
    "    # Chroma STFT\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, chroma_stft))\n",
    "    \n",
    "    # MFCC\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mfcc))\n",
    "    \n",
    "    # Root Mean Square Value\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    result = np.hstack((result, rms))\n",
    "    \n",
    "    # Mel Spectrogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mel))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Function to preprocess and standardize data\n",
    "def preprocess_data(features, scaler):\n",
    "    features = scaler.transform(features.reshape(1, -1))\n",
    "    features = np.expand_dims(features, axis=2)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d43032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion(file_path):\n",
    "    # Load the saved model\n",
    "    model = load_model('emotion_recognition_model.h5')\n",
    "\n",
    "    # Load the scaler used for training\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(pd.read_csv('features.csv').iloc[:, :-1].values)\n",
    "\n",
    "    file_path = file_path\n",
    "\n",
    "    # Load the label encoder used for training\n",
    "    label_encoder = OneHotEncoder()\n",
    "    label_encoder.fit(pd.read_csv('features.csv')['labels'].values.reshape(-1, 1))\n",
    "\n",
    "    # Load and preprocess the audio file\n",
    "    data, sampling_rate = load_audio_file(file_path)\n",
    "    features = extract_features(data, sampling_rate)\n",
    "    processed_features = preprocess_data(features, scaler)\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(processed_features)\n",
    "    predicted_label = np.argmax(prediction, axis=1)\n",
    "\n",
    "    # Map the predicted label to the corresponding emotion\n",
    "    emotion = label_encoder.categories_[0][predicted_label[0]]\n",
    "\n",
    "    return emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b92f4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_country_dict = {\n",
    "    'african': 'Caller assigned to African agent.',\n",
    "    'australia': 'Caller assigned to Australian agent.',\n",
    "    'bermuda': 'Caller assigned to Bermudian agent.',\n",
    "    'canada': 'Caller assigned to Canadian agent.',\n",
    "    'england': 'Caller assigned to English agent.',\n",
    "    'hongkong': 'Caller assigned to Hong Kong agent.',\n",
    "    'indian': 'Caller assigned to Indian agent.',\n",
    "    'ireland': 'Caller assigned to Irish agent.',\n",
    "    'malaysia': 'Caller assigned to Malaysian agent.',\n",
    "    'newzealand': 'Caller assigned to New Zealand agent.',\n",
    "    'philippines': 'Caller assigned to Filipino agent.',\n",
    "    'scotland': 'Caller assigned to Scottish agent.',\n",
    "    'singapore': 'Caller assigned to Singaporean agent.',\n",
    "    'southatlandtic': 'Caller assigned to South Atlantic agent.',\n",
    "    'us': 'Caller assigned to US agent.',\n",
    "    'wales': 'Caller assigned to Welsh agent.'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c1c893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def endorse_to_agent(file_path):\n",
    "    import torch\n",
    "    import torchaudio\n",
    "    from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, pipeline, TrainingArguments, Trainer\n",
    "    \n",
    "    RATE_HZ = 16000\n",
    "    \n",
    "    # Load the saved model\n",
    "    model_name = \"english_accents_classification\"\n",
    "    model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
    "\n",
    "    # Load the feature extractor\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(\"dima806/english_accents_classification\")\n",
    "\n",
    "    # Move the model to the appropriate device (CPU or GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    audio,rate=torchaudio.load(file_path)\n",
    "    transform=torchaudio.transforms.Resample(rate,RATE_HZ)\n",
    "    audio=transform(audio).numpy().reshape(-1)\n",
    "\n",
    "    target_sample_rate = feature_extractor.sampling_rate\n",
    "\n",
    "    # Preprocess the audio\n",
    "    inputs = feature_extractor(audio, sampling_rate=target_sample_rate, return_tensors=\"pt\")\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract logits (raw predictions)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get predicted class\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    predicted_class_label = model.config.id2label[predicted_class_id]\n",
    "\n",
    "    # Print the predicted class\n",
    "    print(f\"Predicted class ID: {predicted_class_id}\")\n",
    "    print(f\"Predicted class label: {predicted_class_label}\")\n",
    "    \n",
    "    return agent_country_dict[predicted_class_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05ea53bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs customer to all checkings if he truly is not angry\n",
    "\n",
    "def verify_customer_is_calm(file_path):\n",
    "    transcript_result = is_caller_angry(file_path)\n",
    "    emotion = get_emotion(file_path)\n",
    "    if emotion == 'angry' or transcript_result == 'OFFENSIVE-LANGUAGE':\n",
    "        endorsement = endorse_to_agent(file_path)\n",
    "        print('customer is angry')\n",
    "        print(endorsement)\n",
    "        return 1\n",
    "    else:\n",
    "        print('customer is calm')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e03d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing. Please wait...\n",
      "Transcript: {'text': ' Kids are talking by the door!'}\n",
      "User is calm.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 627ms/step\n",
      "Predicted class ID: 4\n",
      "Predicted class label: england\n",
      "customer is angry\n",
      "Caller assigned to English agent.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_12/03-01-05-02-01-02-12.wav\"\n",
    "verify_customer_is_calm(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
